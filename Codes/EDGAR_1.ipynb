{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR - Advanced Text Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, we have collected CIKs for each of the Mutual Funds, then looked up the links of all the 13F HRs and the Information Tables, and identified them into text tables. Now, we will obtain information from the text file.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we have to collect the links of the text Info Tables in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "CIK = []\n",
    "text_link = []\n",
    "text_name = []\n",
    "text_date = []\n",
    "\n",
    "input_file = open('HW_Mutual_Fund_Info_Table_Link.csv', 'r')\n",
    "\n",
    "rows = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "# Your Code on Enumerating the Lists.  The result should be three lists, text_link, text_Name,\n",
    "#and text_date.  Each should have length 122.\n",
    "for i in range(1, len(rows)):\n",
    "    columns = rows[i].split(',')\n",
    "      \n",
    "    if columns[4] == \"text\":\n",
    "        CIK.append(columns[0])\n",
    "        text_link.append('https://www.sec.gov' + columns[3])\n",
    "        text_name.append(columns[1])\n",
    "        text_date.append(columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "122\n",
      "122\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "print(len(text_date))\n",
    "print(len(text_link))\n",
    "print(len(text_name))\n",
    "print(len(CIK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only the links that correspond to a date after 2010 (Don't inlude 2010, start at 2011).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Use the following list to store the filtered values.\n",
    "filtered_dates = []\n",
    "filtered_name = []\n",
    "filtered_link = []\n",
    "filtered_cik = []\n",
    "\n",
    "#Enter code here to to keep only the dates corresponding to after 2010.  \n",
    "for i in range(0, 122):\n",
    "    x = datetime.strptime(text_date[i], '%Y-%m-%d')\n",
    "    if (x.year > 2010):\n",
    "        filtered_dates.append(text_date[i])\n",
    "        filtered_name.append(text_name[i])\n",
    "        filtered_link.append(text_link[i])\n",
    "        filtered_cik.append(CIK[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your filtered list should now have 8 elements.  These represent 3 mutual funds.   The first one has CIK=1311981 (from the link you can find this at data/1311981 ).  The second was has CIK 813470.  The third one has CIK 1432353. \n",
    "\n",
    "['https://www.sec.gov/Archives/edgar/data/1311981/000116204413000513/0001162044-13-000513.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000006/0000813470-13-000006.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000001/0000813470-13-000001.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000023/0000813470-12-000023.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000019/0000813470-12-000019.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000014/0000813470-12-000014.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000003/0000813470-12-000003.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/1432353/000114420411008428/0001144204-11-008428.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, for each text link, extract the name of issuer, CUSIP, and the Quantity of shares.  You will also want to keep track of the mutual fund name as well as the filing report date.  \n",
    "\n",
    "#### Your output file should have 5 columns.  The first is the issue date of the form which can be found in the filtered_date list (this will be repeated for the same form).  The second is the mutual fund name which can be found in the filtered_name list (this will be repeated).  The third, fourth and fifith are the name of issuer, CUSIP, and shares respectively.  Make sure to account for the fact that while some of the text files have the same formatting, others do not.  This means you will have to look through them to make sure your code works for the each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store the required data \n",
    "issue_date = []\n",
    "mutual_fund_name = [] \n",
    "name_of_issuer = [] \n",
    "CUSIP = [] \n",
    "shares = [] \n",
    "\n",
    "# Your code goes here\n",
    "import time\n",
    "import random\n",
    "for link_number in range(0,1): # for link 1\n",
    "    html = urllib.request.urlopen(filtered_link[link_number]).read().decode('utf-8') # generate html code\n",
    "    start = html.find('Name of Issuer')\n",
    "    remaining  = html[start:]\n",
    "    rows = remaining.splitlines(1)[2: -9] # find the data rows\n",
    "    for i in range(0, len(rows)): # iterate over rows and append the lists\n",
    "        CUSIP.append(rows[i][45:54])\n",
    "        name_of_issuer.append(rows[i][:30].strip())\n",
    "        shares.append(rows[i][75:82].strip())\n",
    "        mutual_fund_name.append(filtered_name[link_number])\n",
    "        issue_date.append(filtered_dates[link_number])\n",
    "    \n",
    "for link_number in range(1,6): # for links 2-6\n",
    "    html = urllib.request.urlopen(filtered_link[link_number]).read().decode('utf-8') \n",
    "    start = html.find('NAME OF ISSUER')\n",
    "    end = html.find('</TABLE>')\n",
    "    remaining  = html[start:end]\n",
    "    rows = remaining.splitlines(1)[3: -2] # find the data rows\n",
    "    rows.append('\\n')\n",
    "    for i in range(0, len(rows)-1, 2):\n",
    "        mutual_fund_name.append(filtered_name[link_number])\n",
    "        issue_date.append(filtered_dates[link_number])\n",
    "        # check to see if company name continues to next line, concatenate to get full name of company\n",
    "        if (rows[i] != '\\n'):\n",
    "            CUSIP.append(rows[i][37:48])\n",
    "            if rows[i+1] == '\\n':\n",
    "                name_of_issuer.append(rows[i][:20].strip())\n",
    "            else:\n",
    "                x = rows[i][:20].strip() + rows[i+1][:20].strip()\n",
    "                name_of_issuer.append(x)\n",
    "            shares.append(rows[i][63:78].strip())\n",
    "           # try1.append(rows[i+1])\n",
    "        elif len(rows[i+1]) > 25:\n",
    "            CUSIP.append(rows[i][37:48])\n",
    "            x = rows[i][:20].strip() + rows[i+1][:20].strip()\n",
    "            print(x)\n",
    "            name_of_issuer.append(x)\n",
    "            shares.append(rows[i][62:78].strip())\n",
    "\n",
    "\n",
    "for link_number in range(7,8): # for link 8\n",
    "    html = urllib.request.urlopen(filtered_link[link_number]).read().decode('utf-8')\n",
    "    start = html.find('Name of Issuer')\n",
    "    remaining  = html[start:]\n",
    "    rows = remaining.splitlines(1)[3: -5]\n",
    "    for i in range(0, len(rows)):\n",
    "        CUSIP.append(rows[i][50:63])\n",
    "        name_of_issuer.append(rows[i][:30].strip())\n",
    "        shares.append(rows[i][75:85].strip())\n",
    "        mutual_fund_name.append(filtered_name[link_number])\n",
    "        issue_date.append(filtered_dates[link_number])\n",
    "\n",
    "\n",
    "for link_number in range(6,7): # for link 7\n",
    "    html = urllib.request.urlopen(filtered_link[link_number]).read().decode('utf-8')\n",
    "    start = html.find('NAME OF ISSUER')\n",
    "    end = html.find('</TABLE>')\n",
    "    remaining  = html[start:end]        \n",
    "\n",
    "    while html.find('NAME OF ISSUER') != -1: # keep finding tables until -1 is returned\n",
    "        start = html.find('NAME OF ISSUER')\n",
    "        end = html.find('</TABLE>')\n",
    "        remaining  = html[start:end]        \n",
    "        rows = remaining.splitlines(1)[2: ]\n",
    "        header = remaining.splitlines(1)[0: 1]\n",
    "        loc = header[0].find('TITLE OF CLASS') # find data relative to the position of this header field\n",
    "        for i in range(0, len(rows)):\n",
    "            CUSIP.append(rows[i][loc+14:loc+25])\n",
    "            name_of_issuer.append(rows[i][:loc].strip())\n",
    "            shares.append(rows[i][loc+35:loc+42].strip())\n",
    "            mutual_fund_name.append(filtered_name[link_number])\n",
    "            issue_date.append(filtered_dates[link_number])\n",
    "        f = html.find('</SEC-DOCUMENT>')\n",
    "        html = html[end+7: f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, you write down the lists in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# write the scraped data to csv file\n",
    "import csv\n",
    "with open('part1.csv', mode='w') as output_file:\n",
    "    output_writer = csv.writer(output_file, delimiter=',', lineterminator='\\n', quoting = csv.QUOTE_ALL)\n",
    "    output_writer.writerow(['issue_date', 'mutual_fund_name', 'name_of_issuer', 'cusip', 'number_of_shares'])\n",
    "    for x in range(0, len(shares)):\n",
    "        myData = [issue_date[x], mutual_fund_name[x], name_of_issuer[x], CUSIP[x], shares[x]]\n",
    "        output_writer.writerow(myData)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
